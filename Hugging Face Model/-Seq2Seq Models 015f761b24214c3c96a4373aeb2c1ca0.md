# -Seq2Seq Models

# BART FineTuning

- 어조변경 , 번역등 문제 seq2seq문제에 사용됌
- BERT와 BART는 다르다
- BERT는 인코더만 쌓은것
- BART는 인코더와 디코더 포함한것
.md
[BART_Finetuning.ipynb](https://github.com/rlarlgnszx/AI_Study/blob/main/Hugging%20Face%20Model/-Seq2Seq%20Models/BART_Finetuning.ipynb)

# PEGASUS-PreTraining with Extracted Gap-Sentences for abstractive Summarization

### pegasus → 추출된 갭문장을 사용하여 요약하는 사전훈련 모델

- 언어번역 → 요약에 사용
- NLP 요약목적으로 주로 사용
- 중요한 문장은 제거되거나 마스킹되어 예측
- predict highlights
    - 생성

[pegasus.ipynb](https://github.com/rlarlgnszx/AI_Study/blob/main/Hugging%20Face%20Model/-Seq2Seq%20Models/pegasus.ipynb)

# MT5 - Massively multilingual pre-trained t ext-to text transformer

[mT5.ipynb](https://github.com/rlarlgnszx/AI_Study/blob/main/Hugging%20Face%20Model/-Seq2Seq%20Models/mT5.ipynb)

[mT5 (1).ipynb](https://github.com/rlarlgnszx/AI_Study/blob/main/Hugging%20Face%20Model/-Seq2Seq%20Models/mT5_(1).ipynb)