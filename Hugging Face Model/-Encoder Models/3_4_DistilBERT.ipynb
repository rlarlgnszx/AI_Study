{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DRw6CbYV6gWz",
        "outputId": "4e754897-6bc8-4aae-9ebf-bafed28bfb32"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 두 유사도를 얻기위해 cosine 사용\n",
        "### 다른 유사도 측정기법 사용도 가능\n",
        "### Levenstein 거리, 등"
      ],
      "metadata": {
        "id": "-Hn418y5OnjG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DistilBertTokenizer, DistilBertModel\n",
        "from scipy.spatial.distance import cosine\n",
        "import torch"
      ],
      "metadata": {
        "id": "-tJHSXbc68m4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DistilBertModel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwHUocN878gD",
        "outputId": "0d01d87b-bc29-4412-a93a-37468ed44424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "transformers.models.distilbert.modeling_distilbert.DistilBertModel"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_similarity(sentence1, sentence2):\n",
        "    # Load DistilBERT tokenizer and model\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    model = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
        "    print(model)\n",
        "    print(tokenizer)\n",
        "    # Tokenize input sentences\n",
        "    ## 동일한 길이가 아닐때 padding\n",
        "    tokens = tokenizer([sentence1, sentence2], padding=True, truncation=True, return_tensors='pt')\n",
        "\n",
        "    # Get sentence embeddings\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**tokens)\n",
        "        sentence_embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
        "    print(outputs.last_hidden_state)\n",
        "    print(outputs.last_hidden_state.shape)\n",
        "\n",
        "    # Calculate cosine similarity\n",
        "    print(sentence_embeddings.shape)\n",
        "    ## 코사인 유사도를 얻는 방법\n",
        "    ### 두가지의 768==embedding size, vocabsize 를 비교한다.\n",
        "    ### 각기 vector의 유사도를 cosin 유사도를 통해서 비교하면 각기 유사도가 나옴\n",
        "    similarity = 1 - cosine(sentence_embeddings[0], sentence_embeddings[1])\n",
        "    return similarity"
      ],
      "metadata": {
        "id": "p2HGe3Q26luM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = \"I like cats\"\n",
        "sentence2 = \"There are some boys playing football\"\n",
        "sentence3 = 'run in the river side'"
      ],
      "metadata": {
        "id": "4Kxyayu26sD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 뜻이 비슷한것이 아니라 문장의 임베딩과 문맥적 이해면에서 비슷하다는 의미"
      ],
      "metadata": {
        "id": "aJIh9PcgliHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_score = calculate_sentence_similarity(sentence1, sentence3)\n",
        "print(\"Similarity score:\", similarity_score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSBavMJ17A5K",
        "outputId": "9b603245-5e72-4c24-d24b-39509b60bd10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
            "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DistilBertModel(\n",
            "  (embeddings): Embeddings(\n",
            "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (transformer): Transformer(\n",
            "    (layer): ModuleList(\n",
            "      (0-5): 6 x TransformerBlock(\n",
            "        (attention): MultiHeadSelfAttention(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n",
            "        )\n",
            "        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (ffn): FFN(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (activation): GELUActivation()\n",
            "        )\n",
            "        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            ")\n",
            "DistilBertTokenizer(name_or_path='distilbert-base-uncased', vocab_size=30522, model_max_length=512, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}, clean_up_tokenization_spaces=True)\n",
            "tensor([[[-5.7895e-02,  3.7098e-02, -4.3011e-03,  ..., -2.5722e-02,\n",
            "           3.1916e-01,  3.2751e-01],\n",
            "         [ 4.6223e-01,  2.3055e-01, -1.7468e-01,  ...,  6.5499e-02,\n",
            "           4.3582e-01,  4.5284e-01],\n",
            "         [ 5.5888e-01,  4.9665e-01,  7.0609e-01,  ..., -8.1844e-02,\n",
            "           1.3941e-01,  1.5339e-01],\n",
            "         ...,\n",
            "         [ 9.1689e-01,  3.2904e-01, -2.8505e-01,  ...,  1.6665e-01,\n",
            "          -4.3190e-01, -2.7322e-01],\n",
            "         [ 7.1344e-02, -7.2808e-02, -2.1582e-01,  ..., -4.4846e-02,\n",
            "           1.5157e-01,  1.8097e-01],\n",
            "         [ 6.6534e-02, -1.1842e-01, -2.0121e-01,  ...,  3.9689e-02,\n",
            "           1.6502e-01,  2.2740e-01]],\n",
            "\n",
            "        [[-3.2156e-01, -2.4548e-01, -1.1340e-01,  ..., -1.3868e-01,\n",
            "           1.0187e-01,  4.0348e-01],\n",
            "         [ 5.7495e-01, -2.4151e-01, -3.6328e-01,  ..., -2.6485e-01,\n",
            "           1.2447e-01,  4.4576e-01],\n",
            "         [-5.8527e-01, -6.9104e-01, -5.4395e-01,  ..., -3.2988e-01,\n",
            "          -2.1478e-01,  6.4663e-01],\n",
            "         ...,\n",
            "         [-4.0169e-01, -4.6049e-02, -1.8835e-01,  ...,  5.7350e-04,\n",
            "          -2.5318e-01, -1.2463e-01],\n",
            "         [-8.9502e-02, -6.8784e-01, -2.1510e-01,  ..., -2.5206e-02,\n",
            "          -3.7695e-01,  1.7974e-01],\n",
            "         [ 7.8004e-01,  2.1830e-01, -5.2573e-01,  ...,  2.2065e-02,\n",
            "          -5.9888e-01, -3.0695e-01]]])\n",
            "torch.Size([2, 7, 768])\n",
            "(2, 768)\n",
            "Similarity score: 0.9361579418182373\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LTnDhbx85UL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}